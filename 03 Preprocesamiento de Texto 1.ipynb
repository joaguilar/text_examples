{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b38c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "import unidecode\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40ca388",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f1e894f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joseaguilar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joseaguilar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/joseaguilar/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/joseaguilar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Inicializar spaCy para lematización y POS tagging\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8516dd87",
   "metadata": {},
   "source": [
    "# Texto de ejemplo\n",
    "\n",
    "```\n",
    "It's essential to drink coffee at a café and to keep one’s emotions in check while analyzing complex, real-time data!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48171204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto de ejemplo\n",
    "text = \"It's essential to drink coffee at a café and to keep one’s emotions in check while analyzing complex, real-time data!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9971495",
   "metadata": {},
   "source": [
    "# Expansión de contracciones\n",
    "\n",
    "\n",
    "Convertir las formas abreviadas de las palabras, conocidas como contracciones, en sus formas completas. Util para normalizar el texto.\n",
    "\n",
    "## Ejemplo\n",
    "\n",
    "### Texto original\n",
    "- **Oración:** \"I'm going to the store because I can't wait.\"\n",
    "\n",
    "### Después de expandir contracciones\n",
    "- **Resultado:** \"I am going to the store because I cannot wait.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106710ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de la expansión de contracciones: It is essential to drink coffee at a café and to keep one’s emotions in check while analyzing complex, real-time data!\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Expansión de contracciones\n",
    "text = contractions.fix(text)\n",
    "print(\"Después de la expansión de contracciones:\", text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62220853",
   "metadata": {},
   "source": [
    "# Eliminación de Diacríticos\n",
    "\n",
    "La **eliminación de diacríticos** es un proceso en el procesamiento del lenguaje natural que consiste en eliminar los signos diacríticos o acentos de los caracteres. \n",
    "\n",
    "Es útil para normalizar texto donde los acentos pueden no ser relevantes o consistentes en la entrada del usuario.\n",
    "\n",
    "## Ejemplos\n",
    "\n",
    "### Texto original\n",
    "- **Oración:** \"El niño está en el café.\"\n",
    "\n",
    "### Después de eliminar diacríticos\n",
    "- **Resultado:** \"El nino esta en el cafe.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1dc80a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de la eliminación de diacríticos: It is essential to drink coffee at a cafe and to keep one's emotions in check while analyzing complex, real-time data!\n"
     ]
    }
   ],
   "source": [
    "# Paso 2: Eliminación de diacríticos\n",
    "text = unidecode.unidecode(text)\n",
    "print(\"Después de la eliminación de diacríticos:\", text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64ae2e1",
   "metadata": {},
   "source": [
    "# Tokenización y eliminación de puntuación\n",
    "\n",
    "La **tokenización** es el proceso de dividir un texto en unidades más pequeñas llamadas *tokens*. Los tokens pueden ser palabras, frases, u otras unidades significativas del texto. \n",
    "\n",
    "La **eliminación de puntuación** es un proceso que implica quitar todos los signos de puntuación del texto. Esto se realiza para simplificar el texto y enfocarse únicamente en las palabras, ya que la puntuación generalmente no aporta significado en tareas de análisis de contenido.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544f67f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de la tokenización y eliminación de puntuación: ['It', 'is', 'essential', 'to', 'drink', 'coffee', 'at', 'a', 'cafe', 'and', 'to', 'keep', 'one', 'emotions', 'in', 'check', 'while', 'analyzing', 'complex', 'data']\n"
     ]
    }
   ],
   "source": [
    "# Paso 3: Tokenización y eliminación de puntuación\n",
    "tokens = word_tokenize(text)\n",
    "tokens = [token for token in tokens if token.isalpha()]\n",
    "print(\"Después de la tokenización y eliminación de puntuación:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b437a9",
   "metadata": {},
   "source": [
    "# Minúsculas\n",
    "\n",
    "Proceso simple pero crucial. Convierter todas las letras de un texto a su forma minúscula. Este paso ayuda a normalizar los datos de texto, permitiendo que las palabras sean tratadas de manera uniforme, independientemente de cómo estén escritas en el texto original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6998e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de convertir a minúsculas: ['it', 'is', 'essential', 'to', 'drink', 'coffee', 'at', 'a', 'cafe', 'and', 'to', 'keep', 'one', 'emotions', 'in', 'check', 'while', 'analyzing', 'complex', 'data']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Paso 4: Cambio a minúsculas\n",
    "tokens = [token.lower() for token in tokens]\n",
    "print(\"Después de convertir a minúsculas:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e7aac",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "Proceso de eliminar palabras comunes que no aportan un significado significativo al análisis de texto. Incluyen términos como \"el,\" \"la,\" \"es,\" \"en,\" y \"con\" en español, o \"the,\" \"is,\" \"in,\" y \"on\" en inglés.\n",
    "\n",
    "## Ejemplos\n",
    "\n",
    "### Texto original\n",
    "- **Oración:** \"El perro está en el jardín.\"\n",
    "\n",
    "### Después de eliminar stopwords\n",
    "- **Resultado:** \"perro jardín\"\n",
    "\n",
    "### Texto original\n",
    "- **Oración:** \"She is going to the store.\"\n",
    "\n",
    "### Después de eliminar stopwords\n",
    "- **Resultado:** \"going store\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2db97699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de la eliminación de stopwords: ['essential', 'drink', 'coffee', 'cafe', 'keep', 'one', 'emotions', 'check', 'analyzing', 'complex', 'data']\n"
     ]
    }
   ],
   "source": [
    "# Paso 5: Eliminación de stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "print(\"Después de la eliminación de stopwords:\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7735395c",
   "metadata": {},
   "source": [
    "# Lemmatización\n",
    "\n",
    "La **lematización** es un proceso en el procesamiento del lenguaje natural que consiste en reducir las palabras a su forma base o lema. A diferencia del stemming, que simplemente recorta los sufijos, la lematización utiliza un enfoque más inteligente que considera el análisis morfológico de las palabras.\n",
    "\n",
    "## Ejemplos\n",
    "\n",
    "1. **Palabra:** \"running\"  \n",
    "   - **Tipo de palabra:** Verbo  \n",
    "   - **Lema:** \"run\"\n",
    "\n",
    "2. **Palabra:** \"better\"  \n",
    "   - **Tipo de palabra:** Adjetivo  \n",
    "   - **Lema:** \"good\"\n",
    "\n",
    "3. **Palabra:** \"geese\"  \n",
    "   - **Tipo de palabra:** Sustantivo  \n",
    "   - **Lema:** \"goose\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ac7986f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de la lematización: ['essential', 'drink', 'coffee', 'cafe', 'keep', 'one', 'emotion', 'check', 'analyzing', 'complex', 'data']\n"
     ]
    }
   ],
   "source": [
    "# Paso 6: Lematización\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(\"Después de la lematización:\", lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbfe07a",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "El stemming se usa para reducir las palabras a su raiz. Utiliza heuristicas y/o diccionarios para hacerlo.\n",
    "En este caso utilizamos el stemmer de Porter, que es muy eficiente y funciona bastante bien.\n",
    "\n",
    "El **algoritmo de Porter Stemmer** es un proceso para reducir las palabras a su forma base o raíz. Fue desarrollado por Martin Porter en 1980 y es uno de los algoritmos de stemming más conocidos y utilizados en el procesamiento del lenguaje natural. \n",
    "\n",
    "El objetivo del algoritmo es eliminar los sufijos comunes de las palabras en ingléspara que palabras relacionadas se reduzcan a la misma palabra. Por ejemplo, las palabras \"running\", \"runner\", y \"runs\" --> \"run\". Aplica una serie de reglas de sustitución en varias etapas, lo que permite manejar diferentes variaciones y complejidades del idioma.\n",
    "\n",
    "\n",
    "## Ejemplo \"happiness\"\n",
    "\n",
    "### Paso 1: Remover sufijos plurales y \"-ed\" o \"-ing\"\n",
    "- No se aplica ninguna regla.\n",
    "\n",
    "### Paso 2: Remover sufijos \"ational\", \"iveness\", \"fulness\", etc.\n",
    "- **Regla aplicada:** Si la palabra termina en \"ness\", eliminar \"ness\".\n",
    "- **Resultado:** happi\n",
    "\n",
    "### Paso 3: Aplicar reglas de transformación\n",
    "- **Regla aplicada:** Si la palabra termina en \"i\", reemplazar con \"y\".\n",
    "- **Resultado:** happy\n",
    "\n",
    "### Raíz final\n",
    "- **Resultado final:** happi\n",
    "\n",
    "## Ejemplo \"relational\"\n",
    "\n",
    "### Paso 1: Remover sufijos plurales y \"-ed\" o \"-ing\"\n",
    "- No se aplica ninguna regla.\n",
    "\n",
    "### Paso 2: Remover sufijos \"ational\", \"iveness\", \"fulness\", etc.\n",
    "- **Regla aplicada:** Si la palabra termina en \"ational\", reemplazar con \"ate\".\n",
    "- **Resultado:** relate\n",
    "\n",
    "### Paso 3: Aplicar reglas de transformación\n",
    "- No se aplica ninguna regla adicional.\n",
    "\n",
    "### Raíz final\n",
    "- **Resultado final:** relat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a26513c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después del stemming: ['essenti', 'drink', 'coffe', 'cafe', 'keep', 'one', 'emot', 'check', 'analyz', 'complex', 'data']\n"
     ]
    }
   ],
   "source": [
    "# Paso 7: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stems = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(\"Después del stemming:\", stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd2256",
   "metadata": {},
   "source": [
    "# Part of Speech\n",
    "\n",
    "El último paso en este ejemplo es asignar el *Part of Speech* a cada palabra.\n",
    "Los *part of speech* usados son los siguientes:\n",
    "\n",
    "* CC coordinating conjunction\n",
    "* CD cardinal digit\n",
    "* DT determiner\n",
    "* EX existential there (like: “there is” … think of it like “there exists”)\n",
    "* FW foreign word\n",
    "* IN preposition/subordinating conjunction\n",
    "* JJ adjective \"big\"\n",
    "* JJR adjective, comparative \"bigger\"\n",
    "* JJS adjective, superlative \"biggest\"\n",
    "* LS list marker 1)\n",
    "* MD modal could, will\n",
    "* NN noun, singular \"desk\"\n",
    "* NNS noun plural \"desks\"\n",
    "* NNP proper noun, singular \"Harrison\"\n",
    "* NNPS proper noun, plural \"Americans\"\n",
    "* PDT predeterminer \"all the kids\"\n",
    "* POS possessive ending parent\"s\n",
    "* PRP personal pronoun I, he, she\n",
    "* PRP\\$ possessive pronoun my, his, hers\n",
    "* RB adverb very, silently,\n",
    "* RBR adverb, comparative better\n",
    "* RBS adverb, superlative best\n",
    "* RP particle give up\n",
    "* TO, to go \"to\" the store.\n",
    "* UH interjection, errrrrrrrm\n",
    "* VB verb, base form take\n",
    "* VBD verb, past tense took\n",
    "* VBG verb, gerund/present participle taking\n",
    "* VBN verb, past participle taken\n",
    "* VBP verb, sing. present, non-3d take\n",
    "* VBZ verb, 3rd person sing. present takes\n",
    "* WDT wh-determiner which\n",
    "* WP wh-pronoun who, what\n",
    "* WP\\$ possessive wh-pronoun whose\n",
    "* WRB wh-abverb where, when\n",
    "\n",
    "Tags tomadas de [Categorizing and POS Tagging with NLTK Python](https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50fd95e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después del etiquetado de partes del discurso:\n",
      "\n",
      "essential:JJ\n",
      "drink:NN\n",
      "coffee:NN\n",
      "cafe:JJ\n",
      "keep:VB\n",
      "one:CD\n",
      "emotions:NNS\n",
      "check:VB\n",
      "analyzing:VBG\n",
      "complex:JJ\n",
      "data:NNS\n"
     ]
    }
   ],
   "source": [
    "# Paso 8: Part of Speech Tagging\n",
    "pos_tags = nltk.pos_tag(filtered_tokens)\n",
    "print(\"Después del etiquetado de partes del discurso:\\n\")\n",
    "\n",
    "for tag in pos_tags:\n",
    "    print(f\"{tag[0]}:{tag[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa22868",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285bfe34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (text_examples)",
   "language": "python",
   "name": "text_examples_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
